{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MLFLOW_TRACKING_URI\"] = \"https://dagshub.com/AbhijeethKollarapu/datascience_e2e_project1.mlflow\"\n",
    "os.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"AbhijeethKollarapu\"\n",
    "os.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"6b5ffdc1153cff3a2c47770bb59dbd134785df4b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\mlflow\\\\datascience-e2e-project1'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\mlflow\\\\datascience-e2e-project1'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.chdir(\"../\")\n",
    "# %pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "  root_dir: Path\n",
    "  model_path: Path\n",
    "  test_data_path: Path\n",
    "  metrics_file_name: Path\n",
    "  mlflow_tracking_uri: str\n",
    "  target_col: str\n",
    "  model_params: dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.ds_e2e_project1.constants import *\n",
    "from src.ds_e2e_project1.utils.common import read_yaml, create_directories, save_json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "  def __init__(\n",
    "      self,\n",
    "      config_filepath = CONFIG_FILE_PATH,\n",
    "      params_filepath = PARAMS_FILE_PATH,\n",
    "      schema_filepath = SCHEMA_FILE_PATH\n",
    "      ):\n",
    "    self.config = read_yaml(config_filepath)\n",
    "    self.params = read_yaml(params_filepath)\n",
    "    self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "    create_directories([self.config.artifacts_root])\n",
    "\n",
    "  def get_model_eval_config(self) -> ModelEvaluationConfig:\n",
    "    config = self.config.model_evaluation\n",
    "    create_directories([config.root_dir])\n",
    "    model_eval_config = ModelEvaluationConfig(\n",
    "      metrics_file_name=config.metrics_file_name,\n",
    "      mlflow_tracking_uri=os.environ[\"MLFLOW_TRACKING_URI\"],\n",
    "      model_path=config.model_path,\n",
    "      root_dir=config.root_dir,\n",
    "      target_col=self.schema.TARGET_COLUMN.name,\n",
    "      test_data_path=config.test_data_path,\n",
    "      model_params=self.params.ElasticNet\n",
    "    )\n",
    "    return model_eval_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import accuracy_score, r2_score, confusion_matrix, classification_report, mean_squared_error, mean_absolute_error\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature \n",
    "import mlflow.sklearn\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluation:\n",
    "  def __init__(self, model_eval_config: ModelEvaluationConfig):\n",
    "    self.config = model_eval_config\n",
    "\n",
    "  def get_model(self):\n",
    "    model = joblib.load(self.config.model_path)\n",
    "    return model\n",
    "  \n",
    "  def get_test_data(self):\n",
    "    test_data = pd.read_csv(self.config.test_data_path)\n",
    "    X_test = test_data.drop(columns=[self.config.target_col])\n",
    "    y_test = test_data[self.config.target_col]\n",
    "    return X_test, y_test\n",
    "  \n",
    "  def get_evaluation_results(self):\n",
    "    X_test, y_test = self.get_test_data()\n",
    "    model = self.get_model()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # r2 = r2_score(y_test, y_pred)\n",
    "    # mae = mean_absolute_error(y_test, y_pred)\n",
    "    # rmse = (mean_squared_error(y_test, y_pred))**(1/2)\n",
    "    # model_scores = {\"r2\":r2, \"mae\":mae, \"rmse\":rmse}\n",
    "    model_scores = {}\n",
    "    # model_scores[\"accuracy\"] = accuracy_score(y_test, y_pred)\n",
    "    model_scores[\"r2\"] = r2_score(y_test, y_pred)\n",
    "    model_scores[\"mae\"] = mean_absolute_error(y_test, y_pred)\n",
    "    model_scores[\"rmse\"] = (mean_squared_error(y_test, y_pred))**(1/2)\n",
    "    print(model_scores)\n",
    "\n",
    "    save_json_data(dest_json_file_path =  Path(self.config.metrics_file_name), json_data=model_scores)\n",
    "\n",
    "    # cm = confusion_matrix(y_test, y_pred)\n",
    "    # report = classification_report(y_test, y_pred)\n",
    "\n",
    "    # self.log_to_mlflow(model_scores, cm, report, model)\n",
    "\n",
    "    return model_scores\n",
    "\n",
    "  def log_to_mlflow(self):\n",
    "    model = model = self.get_model()\n",
    "    model_scores = self.get_evaluation_results()\n",
    "\n",
    "    X_test, y_test = self.get_test_data()\n",
    "\n",
    "    mlflow.set_tracking_uri(self.config.mlflow_tracking_uri)\n",
    "    mlflow.set_experiment(\"Wine_quality_estimator_ds1\")\n",
    "\n",
    "    with mlflow.start_run():\n",
    "      infer_signature(X_test, y_test)\n",
    "      \n",
    "      # mlflow.log_text(str(cm))\n",
    "      # mlflow.log_text(str(report))\n",
    "\n",
    "      mlflow.log_params(self.config.model_params)\n",
    "      mlflow.log_metrics(model_scores)\n",
    "\n",
    "      tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "      if tracking_url_type_store != \"file\":\n",
    "        mlflow.sklearn.log_model(model, \"model\", registered_model_name=\"Wine_quality_predictor_elasticnet\")\n",
    "      else:\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-03 01:47:40,014 : INFO : common : YAML file config\\config.yaml read successfully]\n",
      "[2025-04-03 01:47:40,017 : INFO : common : YAML file params.yaml read successfully]\n",
      "[2025-04-03 01:47:40,021 : INFO : common : YAML file schema.yaml read successfully]\n",
      "[2025-04-03 01:47:40,022 : INFO : common : Directory artifacts created]\n",
      "[2025-04-03 01:47:40,023 : INFO : common : Directory artifacts/model_evaluation created]\n",
      "{'r2': 0.26038065904013563, 'mae': 0.5394267815902264, 'rmse': 0.6936792109617563}\n",
      "[2025-04-03 01:47:40,134 : INFO : common : Given json data saved to file artifacts\\model_evaluation\\metrics.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mlflow\\datascience-e2e-project1\\ds1-env\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\mlflow\\datascience-e2e-project1\\ds1-env\\Lib\\site-packages\\sklearn\\base.py:380: InconsistentVersionWarning: Trying to unpickle estimator ElasticNet from version 1.5.1 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "d:\\mlflow\\datascience-e2e-project1\\ds1-env\\Lib\\site-packages\\mlflow\\types\\utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/04/03 01:47:46 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'Wine_quality_predictor_elasticnet' already exists. Creating a new version of this model...\n",
      "2025/04/03 01:47:48 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: Wine_quality_predictor_elasticnet, version 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run powerful-kit-590 at: https://dagshub.com/AbhijeethKollarapu/datascience_e2e_project1.mlflow/#/experiments/1/runs/767e1d2942124289b50a97b51e45e0a3\n",
      "🧪 View experiment at: https://dagshub.com/AbhijeethKollarapu/datascience_e2e_project1.mlflow/#/experiments/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '11' of model 'Wine_quality_predictor_elasticnet'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  config_manager = ConfigurationManager()\n",
    "  model_eval_config = config_manager.get_model_eval_config()\n",
    "  model_eval = ModelEvaluation(model_eval_config)\n",
    "  model_eval.log_to_mlflow()\n",
    "except Exception as e:\n",
    "  raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
